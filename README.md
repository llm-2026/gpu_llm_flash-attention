# Flash-Attention in Triton

## Course

The material for this course is in the notebook: [FlashAttention_empty.ipynb](https://github.com/dataflowr/gpu_llm_flash-attention/blob/main/FlashAttention_empty.ipynb)
To run the notebook, you will need access to a GPU.

**Solution 1** If you can create an account on [datalab.sspcloud.fr](https://datalab.sspcloud.fr/), then just click [here](https://datalab.sspcloud.fr/launcher/ide/jupyter-pytorch-gpu?autoLaunch=true&name=flash-attention&init.personalInit=%C2%ABhttps://raw.githubusercontent.com/dataflowr/gpu_llm_flash-attention/refs/heads/main/utils/open-notebook.sh%C2%BB).

**Solution 2** Using Google Colab [here](https://colab.research.google.com/github/dataflowr/gpu_llm_flash-attention/blob/main/FlashAttention_empty.ipynb)
